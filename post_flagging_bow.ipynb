{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b445ee00",
   "metadata": {},
   "source": [
    "In this notebook, we find a reasonable data processing strategy and model for classifying text from a dataset of wikipedia comments (https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data) using a \"bag of words\" approach. The best model tried here (considering both speed and performance) is a multilayer perceptron (MLP) model trained with tensorflow.keras. This matches the recommendation in https://developers.google.com/machine-learning/guides/text-classification/step-2-5 (see the comment a few cells below).\n",
    "\n",
    "Some code based off of the following references:\n",
    "- The book **Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow** by Aurelien Geron\n",
    "- Google Developer's Machine Learning Text Classification Guide (https://developers.google.com/machine-learning/guides/text-classification)\n",
    "- https://github.com/tianqwang/Toxic-Comment-Classification-Challenge/blob/master/Toxic_Comment_Classification.ipynb\n",
    "- https://github.com/scatkinson/is_your_advice_unethical/blob/main/EthicalTips.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b5f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c85eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('wiki_toxic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eff3e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404.92728600944315"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Computes the ratio of number of samples to average number of words per sample.\"\"\"\n",
    "\n",
    "num_samples = len(full_data)\n",
    "mean_wps = full_data['comment_text'].str.len().mean()\n",
    "ratio = num_samples/mean_wps\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f29b3",
   "metadata": {},
   "source": [
    "According to the reference https://developers.google.com/machine-learning/guides/text-classification/step-2-5, for a ratio under 1500, the recommended best data processing strategy is a bag-of-words approach (as opposed to a sequence approach) and the recommended best model is a MLP model. I have not explored the sequence approach yet, but MLP models did indeed seems to perform the best (and were much faster than the next best model type, which was the support vector classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615424c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc91957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ab9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e458c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d36850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"This answers the question: how does toxicity relate to length of the post?\n",
    "Evidently, non-toxic posts are generally longer than toxic posts.\"\"\"\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "full_data['text_length'] = full_data['comment_text'].str.len()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2);\n",
    "sns.boxplot(x='toxic', y='text_length', data=full_data, showfliers=True, ax=ax[0]);\n",
    "sns.boxplot(x='toxic', y='text_length', data=full_data, showfliers=False, ax=ax[1]);\n",
    "ax[0].set_title('Outliers Included');\n",
    "ax[1].set_title('Outliers Removed');\n",
    "for k in [0, 1]:\n",
    "    ax[k].set_xticks([0, 1], ['Not Toxic', 'Toxic']);\n",
    "    ax[k].set_xlabel('');\n",
    "    ax[k].set_ylabel('Text Length');\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2f688bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"As one more interesting analysis step before diving into the vectorization and model-training process,\n",
    "this computes the Flesch-Kincaid grade level for each post and compares the non-toxic/toxic categories on this.\n",
    "Evidently, non-toxic posts are of a higher grade level in general.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level\n",
    "for more information on this score.\"\"\"\n",
    "\n",
    "import textstat\n",
    "\n",
    "if 'grade_level' not in full_data.columns:\n",
    "    full_data['grade_level'] = full_data['comment_text'].apply(textstat.flesch_kincaid_grade)\n",
    "\n",
    "fig, ax = plt.subplots();\n",
    "sns.boxplot(x='toxic', y='grade_level', data=full_data, showfliers=False);\n",
    "ax.set_xticks([0, 1], ['Not Toxic', 'Toxic']);\n",
    "ax.set_xlabel('');\n",
    "ax.set_ylabel('Grade Level');\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156e609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = full_data['comment_text']\n",
    "y_full = full_data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59009b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Randomly shuffle and split the full data set into a training set (80%) and test set (20%).\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1170688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"To keep things running reasonably quickly, we get a sample from the dataset by keeping all\n",
    "the toxic posts and randomly selecting from the non-offensive posts.\n",
    "We then check that the instance indices match the label indices.\"\"\"\n",
    "\n",
    "keep_num=30000\n",
    "\n",
    "X_toxic = X_train[y_train == 1]\n",
    "X_nontoxic = X_train[y_train == 0].sample(n=keep_num - len(X_toxic), random_state=34)\n",
    "X_sampled = pd.concat([X_toxic, X_nontoxic]).sample(frac=1, random_state=33)\n",
    "\n",
    "y_toxic = y_train[y_train == 1]\n",
    "y_nontoxic = y_train[y_train == 0].sample(n=keep_num - len(X_toxic), random_state=34)\n",
    "y_sampled = pd.concat([y_toxic, y_nontoxic]).sample(frac=1, random_state=33)\n",
    "\n",
    "X_sampled.index.equals(y_sampled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f7b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A simple tokenizer for data preprocessing.\"\"\"\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def tokenizer_simple(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text by making everything lowercase, removing punctuation and unallowed symbols,\n",
    "    removing any non-ascii code, and lemmatizing words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    unallowed = '0-9\\r\\t\\n'\n",
    "    regex = re.compile(r'[' + re.escape(string.punctuation) + unallowed +']')\n",
    "    nopunct = regex.sub(' ', text)\n",
    "    words = nopunct.split(' ')\n",
    "    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda28d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Alternate tokenizer with a more sophisticated lemmatizer using POS tagging.\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer_pos(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text by making everything lowercase, removing punctuation and unallowed symbols,\n",
    "    removing any non-ascii code, and lemmatizing words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    unallowed = '0-9\\r\\t\\n'\n",
    "    regex = re.compile(r'[' + re.escape(string.punctuation) + unallowed +']')\n",
    "    nopunct = regex.sub(' ', text)\n",
    "    words = nopunct.split(' ')\n",
    "    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_ for token in doc]\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1df8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A pipeline that vectorizes words and selects the most relevant features.\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "kwargs = {\n",
    "    'strip_accents': 'unicode',\n",
    "    'decode_error': 'replace',\n",
    "    'analyzer': 'word',\n",
    "    'min_df': 2,\n",
    "}\n",
    "\n",
    "vectorize_and_select_pipeline = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer(**kwargs)),\n",
    "    ('select', SelectKBest(f_classif)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1026f188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 20000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"An example of how to set the pipeline's parameters and use it to fit and transform the training set.\"\"\"\n",
    "\n",
    "vectorize_and_select_pipeline.set_params(vectorize__tokenizer=tokenizer_simple,\n",
    "                                         vectorize__ngram_range=(1,2),\n",
    "                                         select__k=20000)\n",
    "\n",
    "X_train_v = vectorize_and_select_pipeline.fit_transform(X_sampled, y_sampled)\n",
    "\n",
    "X_train_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f29b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A function that gets clean predicted labels given a model, (vectorized) training data, and a name that describes\n",
    "the model, then prints the model name and corresponding confusion matrix, precision, recall, and F_1 scores.\"\"\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve\n",
    "\n",
    "def predict_eval(model, X_train_v, y_train, model_name=''):\n",
    "    y_train_pred = cross_val_predict(model, X_train_v, y_train, cv=3)\n",
    "    print(model_name, '\\n',\n",
    "          'Confusion Matrix:\\n', confusion_matrix(y_train, y_train_pred), '\\n\\n',\n",
    "          'Precision: ', precision_score(y_train, y_train_pred), '\\n',\n",
    "          'Recall:    ', recall_score(y_train, y_train_pred), '\\n',\n",
    "          'F_1:       ', f1_score(y_train, y_train_pred),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4635e4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram range: (1, 1)\n",
      "tokenizer: tokenizer_simple\n",
      "top_k: 15000\n",
      "Data prep time: 12.043896913528442\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17018   744]\n",
      " [ 2580  9658]] \n",
      "\n",
      " Precision:  0.9284752932128437 \n",
      " Recall:     0.789181238764504 \n",
      " F_1:        0.8531802120141343\n",
      "\n",
      "Model cross validation time: 0.08217692375183105\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16867   895]\n",
      " [ 2265  9973]] \n",
      "\n",
      " Precision:  0.9176481413323518 \n",
      " Recall:     0.8149207386827914 \n",
      " F_1:        0.8632389855448802\n",
      "\n",
      "Model cross validation time: 0.7854430675506592\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16930   832]\n",
      " [ 2021 10217]] \n",
      "\n",
      " Precision:  0.9246990677889402 \n",
      " Recall:     0.8348586370321948 \n",
      " F_1:        0.8774852922231288\n",
      "\n",
      "Model cross validation time: 487.45681595802307\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[16892   870]\n",
      " [ 3209  9029]] \n",
      "\n",
      " Precision:  0.9121123345792504 \n",
      " Recall:     0.7377839516260827 \n",
      " F_1:        0.8157383565975515\n",
      "\n",
      "Model cross validation time: 82.06661200523376\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16379  1383]\n",
      " [ 3203  9035]] \n",
      "\n",
      " Precision:  0.8672489921290075 \n",
      " Recall:     0.7382742278150024 \n",
      " F_1:        0.7975812146892656\n",
      "\n",
      "Model cross validation time: 19.97251009941101\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16767   995]\n",
      " [ 2519  9719]] \n",
      "\n",
      " Precision:  0.9071308568228487 \n",
      " Recall:     0.7941657133518549 \n",
      " F_1:        0.8468978738236319\n",
      "\n",
      "Model cross validation time: 16.116463899612427\n",
      "\n",
      "ngram range: (1, 1)\n",
      "tokenizer: tokenizer_simple\n",
      "top_k: 20000\n",
      "Data prep time: 10.15030026435852\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17002   760]\n",
      " [ 2772  9466]] \n",
      "\n",
      " Precision:  0.9256796401329943 \n",
      " Recall:     0.7734924007190718 \n",
      " F_1:        0.8427706552706553\n",
      "\n",
      "Model cross validation time: 0.07287120819091797\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16848   914]\n",
      " [ 2299  9939]] \n",
      "\n",
      " Precision:  0.9157836542891367 \n",
      " Recall:     0.8121425069455793 \n",
      " F_1:        0.8608548785241004\n",
      "\n",
      "Model cross validation time: 1.2083849906921387\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16906   856]\n",
      " [ 2112 10126]] \n",
      "\n",
      " Precision:  0.9220542706246585 \n",
      " Recall:     0.8274227815002452 \n",
      " F_1:        0.8721791559000862\n",
      "\n",
      "Model cross validation time: 545.4812967777252\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[16832   930]\n",
      " [ 3222  9016]] \n",
      "\n",
      " Precision:  0.9064950733963403 \n",
      " Recall:     0.7367216865500898 \n",
      " F_1:        0.8128380815001803\n",
      "\n",
      "Model cross validation time: 97.96586608886719\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16379  1383]\n",
      " [ 3203  9035]] \n",
      "\n",
      " Precision:  0.8672489921290075 \n",
      " Recall:     0.7382742278150024 \n",
      " F_1:        0.7975812146892656\n",
      "\n",
      "Model cross validation time: 26.32339906692505\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16758  1004]\n",
      " [ 2510  9728]] \n",
      "\n",
      " Precision:  0.9064480059634737 \n",
      " Recall:     0.7949011276352345 \n",
      " F_1:        0.8470178493687419\n",
      "\n",
      "Model cross validation time: 18.120906829833984\n",
      "\n",
      "ngram range: (1, 1)\n",
      "tokenizer: tokenizer_pos\n",
      "top_k: 15000\n",
      "Data prep time: 385.4167196750641\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17069   693]\n",
      " [ 2614  9624]] \n",
      "\n",
      " Precision:  0.9328293108461763 \n",
      " Recall:     0.786403007027292 \n",
      " F_1:        0.8533806251385502\n",
      "\n",
      "Model cross validation time: 0.07269906997680664\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16840   922]\n",
      " [ 2266  9972]] \n",
      "\n",
      " Precision:  0.9153662566550395 \n",
      " Recall:     0.814839025984638 \n",
      " F_1:        0.8621822583434203\n",
      "\n",
      "Model cross validation time: 0.5895979404449463\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16926   836]\n",
      " [ 1959 10279]] \n",
      "\n",
      " Precision:  0.9247863247863248 \n",
      " Recall:     0.839924824317699 \n",
      " F_1:        0.8803151629340984\n",
      "\n",
      "Model cross validation time: 463.25866293907166\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[16946   816]\n",
      " [ 3245  8993]] \n",
      "\n",
      " Precision:  0.9168110918544194 \n",
      " Recall:     0.7348422944925641 \n",
      " F_1:        0.8158026035288248\n",
      "\n",
      "Model cross validation time: 88.81153416633606\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16372  1390]\n",
      " [ 3080  9158]] \n",
      "\n",
      " Precision:  0.8682214637846037 \n",
      " Recall:     0.7483248896878575 \n",
      " F_1:        0.8038269112613008\n",
      "\n",
      "Model cross validation time: 22.78316593170166\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16791   971]\n",
      " [ 2519  9719]] \n",
      "\n",
      " Precision:  0.9091674462114125 \n",
      " Recall:     0.7941657133518549 \n",
      " F_1:        0.8477843684577809\n",
      "\n",
      "Model cross validation time: 17.11201000213623\n",
      "\n",
      "ngram range: (1, 1)\n",
      "tokenizer: tokenizer_pos\n",
      "top_k: 20000\n",
      "Data prep time: 380.72967886924744\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17007   755]\n",
      " [ 2831  9407]] \n",
      "\n",
      " Precision:  0.9257036016532179 \n",
      " Recall:     0.7686713515280275 \n",
      " F_1:        0.8399107142857143\n",
      "\n",
      "Model cross validation time: 0.0777122974395752\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16820   942]\n",
      " [ 2297  9941]] \n",
      "\n",
      " Precision:  0.9134429844711937 \n",
      " Recall:     0.812305932341886 \n",
      " F_1:        0.8599109035076339\n",
      "\n",
      "Model cross validation time: 0.7252359390258789\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16917   845]\n",
      " [ 2094 10144]] \n",
      "\n",
      " Precision:  0.9231049231049231 \n",
      " Recall:     0.8288936100670045 \n",
      " F_1:        0.8734662246523444\n",
      "\n",
      "Model cross validation time: 518.8454360961914\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[16957   805]\n",
      " [ 3323  8915]] \n",
      "\n",
      " Precision:  0.9171810699588477 \n",
      " Recall:     0.7284687040366072 \n",
      " F_1:        0.8120047363147828\n",
      "\n",
      "Model cross validation time: 96.53631591796875\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16372  1390]\n",
      " [ 3080  9158]] \n",
      "\n",
      " Precision:  0.8682214637846037 \n",
      " Recall:     0.7483248896878575 \n",
      " F_1:        0.8038269112613008\n",
      "\n",
      "Model cross validation time: 27.24268412590027\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16787   975]\n",
      " [ 2494  9744]] \n",
      "\n",
      " Precision:  0.9090400223901484 \n",
      " Recall:     0.7962085308056872 \n",
      " F_1:        0.8488914056714729\n",
      "\n",
      "Model cross validation time: 16.73906898498535\n",
      "\n",
      "ngram range: (1, 2)\n",
      "tokenizer: tokenizer_simple\n",
      "top_k: 15000\n",
      "Data prep time: 12.875719785690308\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17224   538]\n",
      " [ 2776  9462]] \n",
      "\n",
      " Precision:  0.9462 \n",
      " Recall:     0.7731655499264586 \n",
      " F_1:        0.8509758071769045\n",
      "\n",
      "Model cross validation time: 0.07934784889221191\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16864   898]\n",
      " [ 2515  9723]] \n",
      "\n",
      " Precision:  0.9154505225496657 \n",
      " Recall:     0.794492564144468 \n",
      " F_1:        0.8506933811627805\n",
      "\n",
      "Model cross validation time: 0.773529052734375\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16926   836]\n",
      " [ 1913 10325]] \n",
      "\n",
      " Precision:  0.9250963175342711 \n",
      " Recall:     0.8436836084327505 \n",
      " F_1:        0.8825163468524295\n",
      "\n",
      "Model cross validation time: 559.0182282924652\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[16935   827]\n",
      " [ 3300  8938]] \n",
      "\n",
      " Precision:  0.9153097798259089 \n",
      " Recall:     0.730348096094133 \n",
      " F_1:        0.8124346679998182\n",
      "\n",
      "Model cross validation time: 80.13211917877197\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16324  1438]\n",
      " [ 3107  9131]] \n",
      "\n",
      " Precision:  0.8639417163402403 \n",
      " Recall:     0.7461186468377186 \n",
      " F_1:        0.8007190774762135\n",
      "\n",
      "Model cross validation time: 15.643180131912231\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16779   983]\n",
      " [ 2498  9740]] \n",
      "\n",
      " Precision:  0.9083278933134384 \n",
      " Recall:     0.795881680013074 \n",
      " F_1:        0.8483951047428248\n",
      "\n",
      "Model cross validation time: 18.976133823394775\n",
      "\n",
      "ngram range: (1, 2)\n",
      "tokenizer: tokenizer_simple\n",
      "top_k: 20000\n",
      "Data prep time: 12.724750995635986\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17243   519]\n",
      " [ 2825  9413]] \n",
      "\n",
      " Precision:  0.9477446637132501 \n",
      " Recall:     0.7691616277169472 \n",
      " F_1:        0.8491655390166892\n",
      "\n",
      "Model cross validation time: 0.08421683311462402\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16861   901]\n",
      " [ 2503  9735]] \n",
      "\n",
      " Precision:  0.9152877021436631 \n",
      " Recall:     0.7954731165223076 \n",
      " F_1:        0.8511847512459562\n",
      "\n",
      "Model cross validation time: 0.9070460796356201\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16948   814]\n",
      " [ 1925 10313]] \n",
      "\n",
      " Precision:  0.9268446122045475 \n",
      " Recall:     0.8427030560549109 \n",
      " F_1:        0.8827733789856623\n",
      "\n",
      "Model cross validation time: 630.9684438705444\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[17028   734]\n",
      " [ 3358  8880]] \n",
      "\n",
      " Precision:  0.9236530060328687 \n",
      " Recall:     0.725608759601242 \n",
      " F_1:        0.8127402526084568\n",
      "\n",
      "Model cross validation time: 87.22952198982239\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16324  1438]\n",
      " [ 3107  9131]] \n",
      "\n",
      " Precision:  0.8639417163402403 \n",
      " Recall:     0.7461186468377186 \n",
      " F_1:        0.8007190774762135\n",
      "\n",
      "Model cross validation time: 18.89301300048828\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16780   982]\n",
      " [ 2522  9716]] \n",
      "\n",
      " Precision:  0.9082071415217797 \n",
      " Recall:     0.793920575257395 \n",
      " F_1:        0.8472270666201606\n",
      "\n",
      "Model cross validation time: 20.960020780563354\n",
      "\n",
      "ngram range: (1, 2)\n",
      "tokenizer: tokenizer_pos\n",
      "top_k: 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prep time: 392.1509487628937\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17244   518]\n",
      " [ 2812  9426]] \n",
      "\n",
      " Precision:  0.9479082864038616 \n",
      " Recall:     0.77022389279294 \n",
      " F_1:        0.8498782796862321\n",
      "\n",
      "Model cross validation time: 0.08181095123291016\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16852   910]\n",
      " [ 2465  9773]] \n",
      "\n",
      " Precision:  0.9148179350369746 \n",
      " Recall:     0.7985781990521327 \n",
      " F_1:        0.8527551153963614\n",
      "\n",
      "Model cross validation time: 0.6086640357971191\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16957   805]\n",
      " [ 1868 10370]] \n",
      "\n",
      " Precision:  0.9279642058165548 \n",
      " Recall:     0.8473606798496487 \n",
      " F_1:        0.8858326570708582\n",
      "\n",
      "Model cross validation time: 553.7879528999329\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[17054   708]\n",
      " [ 3323  8915]] \n",
      "\n",
      " Precision:  0.9264262703938481 \n",
      " Recall:     0.7284687040366072 \n",
      " F_1:        0.8156077032157723\n",
      "\n",
      "Model cross validation time: 76.45986199378967\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16344  1418]\n",
      " [ 3054  9184]] \n",
      "\n",
      " Precision:  0.8662516506319562 \n",
      " Recall:     0.7504494198398431 \n",
      " F_1:        0.8042031523642731\n",
      "\n",
      "Model cross validation time: 16.253262042999268\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16790   972]\n",
      " [ 2459  9779]] \n",
      "\n",
      " Precision:  0.9095898055994791 \n",
      " Recall:     0.7990684752410525 \n",
      " F_1:        0.8507547087737615\n",
      "\n",
      "Model cross validation time: 18.695822954177856\n",
      "\n",
      "ngram range: (1, 2)\n",
      "tokenizer: tokenizer_pos\n",
      "top_k: 20000\n",
      "Data prep time: 380.8681950569153\n",
      "\n",
      "Naive Bayes \n",
      " Confusion Matrix:\n",
      " [[17256   506]\n",
      " [ 2813  9425]] \n",
      "\n",
      " Precision:  0.9490484341959521 \n",
      " Recall:     0.7701421800947867 \n",
      " F_1:        0.8502864360142541\n",
      "\n",
      "Model cross validation time: 0.07952880859375\n",
      "\n",
      "Logistic \n",
      " Confusion Matrix:\n",
      " [[16860   902]\n",
      " [ 2461  9777]] \n",
      "\n",
      " Precision:  0.915535162468396 \n",
      " Recall:     0.7989050498447459 \n",
      " F_1:        0.8532530435920931\n",
      "\n",
      "Model cross validation time: 0.908087968826294\n",
      "\n",
      "Support Vector \n",
      " Confusion Matrix:\n",
      " [[16941   821]\n",
      " [ 1897 10341]] \n",
      "\n",
      " Precision:  0.9264468733201935 \n",
      " Recall:     0.8449910116032031 \n",
      " F_1:        0.883846153846154\n",
      "\n",
      "Model cross validation time: 606.7833099365234\n",
      "\n",
      "Random Forest \n",
      " Confusion Matrix:\n",
      " [[17060   702]\n",
      " [ 3441  8797]] \n",
      "\n",
      " Precision:  0.9260974839456785 \n",
      " Recall:     0.7188266056545187 \n",
      " F_1:        0.8094033215255096\n",
      "\n",
      "Model cross validation time: 79.59352016448975\n",
      "\n",
      "AdaBoost \n",
      " Confusion Matrix:\n",
      " [[16344  1418]\n",
      " [ 3054  9184]] \n",
      "\n",
      " Precision:  0.8662516506319562 \n",
      " Recall:     0.7504494198398431 \n",
      " F_1:        0.8042031523642731\n",
      "\n",
      "Model cross validation time: 18.53584909439087\n",
      "\n",
      "XGBoost \n",
      " Confusion Matrix:\n",
      " [[16801   961]\n",
      " [ 2474  9764]] \n",
      "\n",
      " Precision:  0.9103962703962704 \n",
      " Recall:     0.797842784768753 \n",
      " F_1:        0.8504115315943037\n",
      "\n",
      "Model cross validation time: 20.852861881256104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A grid search that tries out a variety of models on various text prep strategies,\n",
    "and also keeps track of how long each text prep step and classifier takes.\n",
    "(This is perhaps overkill, but I was curious to see.)\"\"\"\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "lg_clf = LogisticRegression(max_iter=200)\n",
    "sv_clf = SVC()\n",
    "rf_clf = RandomForestClassifier()\n",
    "ada_clf = AdaBoostClassifier()\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "classifiers = {'Naive Bayes': nb_clf,\n",
    "               'Logistic': lg_clf,\n",
    "               'Support Vector': sv_clf,\n",
    "               'Random Forest': rf_clf,\n",
    "               'AdaBoost': ada_clf,\n",
    "               'XGBoost': xgb_clf,\n",
    "              }\n",
    "\n",
    "kwargs = {\n",
    "    'strip_accents': 'unicode',\n",
    "    'decode_error': 'replace',\n",
    "    'analyzer': 'word',\n",
    "    'min_df': 2,\n",
    "}\n",
    "\n",
    "ngram_choices = [(1,1), (1,2)]\n",
    "tokenizer_choices = [tokenizer_simple, tokenizer_pos]\n",
    "top_k_choices = [15000, 20000]\n",
    "\n",
    "temp_time = time.time()\n",
    "\n",
    "for ngram_range in ngram_choices:\n",
    "    for tokenizer in tokenizer_choices:\n",
    "        for top_k in top_k_choices:\n",
    "            print('ngram range: ' + str(ngram_range))\n",
    "            print('tokenizer: ' + tokenizer.__name__)\n",
    "            print('top_k: ' + str(top_k))\n",
    "            #The try-except here is to deal with the possibility that there are less than top_k features after vectorizing.\n",
    "            try:\n",
    "                vectorize_and_select_pipeline.set_params(vectorize__ngram_range=ngram_range,\n",
    "                                                         vectorize__tokenizer=tokenizer,\n",
    "                                                         select__k=top_k)\n",
    "                X_train_v = vectorize_and_select_pipeline.fit_transform(X_sampled, y_sampled)\n",
    "            except ValueError:\n",
    "                vectorize_and_select_pipeline.set_params(vectorize__ngram_range=ngram_range,\n",
    "                                                         vectorize__tokenizer=tokenizer,\n",
    "                                                         select__k='all')\n",
    "                X_train_v = vectorize_and_select_pipeline.fit_transform(X_sampled, y_sampled)\n",
    "            elapsed_time = time.time() - temp_time\n",
    "            temp_time = time.time()\n",
    "            print('Data prep time: ' + str(elapsed_time) +'\\n')\n",
    "            for model_name, model in classifiers.items():\n",
    "                predict_eval(model, X_train_v, y_sampled, model_name=model_name)                \n",
    "                elapsed_time = time.time() - temp_time\n",
    "                temp_time = time.time()\n",
    "                print('\\n' + 'Model cross validation time: ' + str(elapsed_time) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a64fd5",
   "metadata": {},
   "source": [
    "We now explore some MLP neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e3ddbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"First, build the vectorized training set using the a reasonable prep strategy based on the grid search above.\n",
    "Note that we also convert X_train_v to a numpy array so that it is compatible with keras.\"\"\"\n",
    "\n",
    "vectorize_and_select_pipeline.set_params(vectorize__ngram_range=(1,2),\n",
    "                                         vectorize__tokenizer=tokenizer_simple,\n",
    "                                         select__k=20000)\n",
    "X_train_v = vectorize_and_select_pipeline.fit_transform(X_sampled, y_sampled)\n",
    "X_train_v = X_train_v.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d01554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function that builds an MLP model for various parameters.\"\"\"\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_mlp_model(n_hidden=3, n_neurons=30, dropout_rate=0.3, input_shape=20000,):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(units=n_neurons, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5983d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.4268 - accuracy: 0.8057\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2811 - accuracy: 0.8905\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2541 - accuracy: 0.9007\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2408 - accuracy: 0.9090\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2310 - accuracy: 0.9112\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2247 - accuracy: 0.9153\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.4303 - accuracy: 0.8037\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2915 - accuracy: 0.8834\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2625 - accuracy: 0.8971\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2427 - accuracy: 0.9071\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2304 - accuracy: 0.9114\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2294 - accuracy: 0.9119\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.4422 - accuracy: 0.7983\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2885 - accuracy: 0.8849\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2616 - accuracy: 0.8988\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2427 - accuracy: 0.9075\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2372 - accuracy: 0.9079\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2217 - accuracy: 0.9158\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "MLP \n",
      " Confusion Matrix:\n",
      " [[16154  1608]\n",
      " [  950 11288]] \n",
      "\n",
      " Precision:  0.8753101736972705 \n",
      " Recall:     0.9223729367543716 \n",
      " F_1:        0.8982255112596482\n"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(model=build_mlp_model,\n",
    "                            epochs=6,\n",
    "                            verbose=1)\n",
    "\n",
    "predict_eval(keras_clf, X_train_v, y_sampled, model_name='MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e977308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 09:40:39.076981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/3d/8nmtnnsj0n53170j0cb2mtdr0000gn/T/tmpcnp2q12w/assets\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 23s 34ms/step - loss: 0.3695 - accuracy: 0.8395\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.2631 - accuracy: 0.8972\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.2379 - accuracy: 0.9097\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.2390 - accuracy: 0.9086\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 22s 34ms/step - loss: 0.2280 - accuracy: 0.9143\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.2281 - accuracy: 0.9155\n",
      "313/313 [==============================] - 2s 6ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/3d/8nmtnnsj0n53170j0cb2mtdr0000gn/T/tmp7l33xstd/assets\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 22s 33ms/step - loss: 0.3715 - accuracy: 0.8340\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2650 - accuracy: 0.8937\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2418 - accuracy: 0.9065\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2324 - accuracy: 0.9146\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.2318 - accuracy: 0.9145\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2224 - accuracy: 0.9178\n",
      "313/313 [==============================] - 2s 6ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/3d/8nmtnnsj0n53170j0cb2mtdr0000gn/T/tmpkiypdo9e/assets\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 23s 34ms/step - loss: 0.3760 - accuracy: 0.8345\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2706 - accuracy: 0.8946\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.2484 - accuracy: 0.9062\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.2346 - accuracy: 0.9137\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2306 - accuracy: 0.9139\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.2297 - accuracy: 0.9158\n",
      "313/313 [==============================] - 2s 6ms/step\n",
      "Bigger MLP \n",
      " Confusion Matrix:\n",
      " [[16499  1263]\n",
      " [ 1250 10988]] \n",
      "\n",
      " Precision:  0.8969063749897968 \n",
      " Recall:     0.8978591273083837 \n",
      " F_1:        0.897382498264527\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Again, but a bigger model.\"\"\"\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(model=build_mlp_model(n_hidden=5, n_neurons=100),\n",
    "                            epochs=6,\n",
    "                            verbose=1)\n",
    "\n",
    "predict_eval(keras_clf, X_train_v, y_sampled, model_name='Bigger MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37cdb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Similarly to the above, a function that builds an MLP model, but this time with different\n",
    "configuration parameters (based on the default suggestion in HOML).\"\"\"\n",
    "\n",
    "def build_mlp_model_2(n_hidden=3, n_neurons=30, dropout_rate=0.1,\n",
    "                    kernel_initializer='he_normal', activation='elu', input_shape=20000,):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(units=n_neurons, kernel_initializer=kernel_initializer,\n",
    "                        activation=activation))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a8ee41f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "625/625 [==============================] - 7s 10ms/step - loss: 0.3020 - accuracy: 0.8718\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2101 - accuracy: 0.9154\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1918 - accuracy: 0.9241\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1824 - accuracy: 0.9291\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1750 - accuracy: 0.9309\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1699 - accuracy: 0.9331\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3001 - accuracy: 0.8686\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2076 - accuracy: 0.9154\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.1887 - accuracy: 0.9248\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1778 - accuracy: 0.9293\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1738 - accuracy: 0.9297\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1662 - accuracy: 0.9361\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.3108 - accuracy: 0.8659\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2145 - accuracy: 0.9155\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1929 - accuracy: 0.9241\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.1835 - accuracy: 0.9276\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.1787 - accuracy: 0.9317\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.1724 - accuracy: 0.9351\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "MLP new config \n",
      " Confusion Matrix:\n",
      " [[16621  1141]\n",
      " [ 1483 10755]] \n",
      "\n",
      " Precision:  0.9040854068594486 \n",
      " Recall:     0.8788200686386665 \n",
      " F_1:        0.8912737217203945\n"
     ]
    }
   ],
   "source": [
    "keras_clf = KerasClassifier(model=build_mlp_model_2,\n",
    "                            epochs=6,\n",
    "                            verbose=1)\n",
    "\n",
    "predict_eval(keras_clf, X_train_v, y_sampled, model_name='MLP new config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3a370f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/50/rdh34pg55352g7f55r28b2wm0000gq/T/tmpreplembs/assets\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 15s 22ms/step - loss: 0.2861 - accuracy: 0.8832\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2109 - accuracy: 0.9162\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.1931 - accuracy: 0.9237\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1847 - accuracy: 0.9293\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.1800 - accuracy: 0.9288\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.1742 - accuracy: 0.9325\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/50/rdh34pg55352g7f55r28b2wm0000gq/T/tmpoyq8hiqo/assets\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.2815 - accuracy: 0.8820\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.2093 - accuracy: 0.9171\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.1901 - accuracy: 0.9252\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.1793 - accuracy: 0.9299\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.1739 - accuracy: 0.9340\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.1700 - accuracy: 0.9334\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/50/rdh34pg55352g7f55r28b2wm0000gq/T/tmph13pb03b/assets\n",
      "Epoch 1/6\n",
      "625/625 [==============================] - 14s 21ms/step - loss: 0.2895 - accuracy: 0.8770\n",
      "Epoch 2/6\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.2109 - accuracy: 0.9168\n",
      "Epoch 3/6\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1917 - accuracy: 0.9258\n",
      "Epoch 4/6\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1815 - accuracy: 0.9288\n",
      "Epoch 5/6\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.1737 - accuracy: 0.9333\n",
      "Epoch 6/6\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.1716 - accuracy: 0.9345\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Bigger MLP new config \n",
      " Confusion Matrix:\n",
      " [[16462  1300]\n",
      " [ 1310 10928]] \n",
      "\n",
      " Precision:  0.8936866208701341 \n",
      " Recall:     0.8929563654191861 \n",
      " F_1:        0.8933213439058285\n"
     ]
    }
   ],
   "source": [
    "keras_clf = KerasClassifier(model=build_mlp_model_2(n_hidden=5, n_neurons=100),\n",
    "                            epochs=6,\n",
    "                            verbose=1)\n",
    "\n",
    "predict_eval(keras_clf, X_train_v, y_sampled, model_name='Bigger MLP new config')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467ff14",
   "metadata": {},
   "source": [
    "Based on the above, considering F_1 scores and speed, it looks like a reasonable strategy is: prep with tokenizer_simple, ngram_range (1,2), best 20000 features, and a 3 hidden levels, 30 neurons MLP, trained with more than 6 epochs (with early stopping and callback to the best model using a validation set). We could do a more extensive grid search on the best MLP, but precision and recall around 0.9, as seen with cross validating above, seems satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33365ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prep the full training set for training.\"\"\"\n",
    "\n",
    "vectorize_and_select_pipeline.set_params(vectorize__ngram_range=(1,2),\n",
    "                                         vectorize__tokenizer=tokenizer_simple,\n",
    "                                         select__k=20000)\n",
    "X_train_v = vectorize_and_select_pipeline.fit_transform(X_train, y_train)\n",
    "X_train_v = X_train_v.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b4cf709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3192/3192 [==============================] - 59s 18ms/step - loss: 0.2285 - accuracy: 0.9236 - val_loss: 0.1603 - val_accuracy: 0.9487\n",
      "Epoch 2/50\n",
      "3192/3192 [==============================] - 50s 16ms/step - loss: 0.1828 - accuracy: 0.9441 - val_loss: 0.1408 - val_accuracy: 0.9572\n",
      "Epoch 3/50\n",
      "3192/3192 [==============================] - 50s 16ms/step - loss: 0.1706 - accuracy: 0.9497 - val_loss: 0.1347 - val_accuracy: 0.9585\n",
      "Epoch 4/50\n",
      "3192/3192 [==============================] - 50s 16ms/step - loss: 0.1654 - accuracy: 0.9519 - val_loss: 0.1338 - val_accuracy: 0.9597\n",
      "Epoch 5/50\n",
      "3192/3192 [==============================] - 53s 17ms/step - loss: 0.1621 - accuracy: 0.9533 - val_loss: 0.1335 - val_accuracy: 0.9601\n",
      "Epoch 6/50\n",
      "3192/3192 [==============================] - 53s 17ms/step - loss: 0.1576 - accuracy: 0.9538 - val_loss: 0.1297 - val_accuracy: 0.9601\n",
      "Epoch 7/50\n",
      "3192/3192 [==============================] - 55s 17ms/step - loss: 0.1593 - accuracy: 0.9540 - val_loss: 0.1266 - val_accuracy: 0.9604\n",
      "Epoch 8/50\n",
      "3192/3192 [==============================] - 53s 17ms/step - loss: 0.1582 - accuracy: 0.9545 - val_loss: 0.1319 - val_accuracy: 0.9608\n",
      "Epoch 9/50\n",
      "3192/3192 [==============================] - 58s 18ms/step - loss: 0.1573 - accuracy: 0.9547 - val_loss: 0.1312 - val_accuracy: 0.9611\n",
      "Epoch 10/50\n",
      "3192/3192 [==============================] - 57s 18ms/step - loss: 0.1569 - accuracy: 0.9538 - val_loss: 0.1297 - val_accuracy: 0.9610\n",
      "Epoch 11/50\n",
      "3192/3192 [==============================] - 54s 17ms/step - loss: 0.1554 - accuracy: 0.9549 - val_loss: 0.1283 - val_accuracy: 0.9609\n",
      "Epoch 12/50\n",
      "3192/3192 [==============================] - 54s 17ms/step - loss: 0.1568 - accuracy: 0.9547 - val_loss: 0.1342 - val_accuracy: 0.9610\n",
      "Epoch 13/50\n",
      "3192/3192 [==============================] - 54s 17ms/step - loss: 0.1562 - accuracy: 0.9538 - val_loss: 0.1309 - val_accuracy: 0.9604\n",
      "Epoch 14/50\n",
      "3192/3192 [==============================] - 55s 17ms/step - loss: 0.1578 - accuracy: 0.9542 - val_loss: 0.1317 - val_accuracy: 0.9616\n",
      "Epoch 15/50\n",
      "3192/3192 [==============================] - 55s 17ms/step - loss: 0.1571 - accuracy: 0.9547 - val_loss: 0.1355 - val_accuracy: 0.9608\n",
      "Epoch 16/50\n",
      "3192/3192 [==============================] - 55s 17ms/step - loss: 0.1576 - accuracy: 0.9538 - val_loss: 0.1311 - val_accuracy: 0.9612\n",
      "Epoch 17/50\n",
      "3192/3192 [==============================] - 56s 17ms/step - loss: 0.1560 - accuracy: 0.9545 - val_loss: 0.1334 - val_accuracy: 0.9607\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train and save the model.\"\"\"\n",
    "\n",
    "model = build_mlp_model(n_hidden=3, n_neurons=30, dropout_rate=0.3, input_shape=20000,)\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('keras_model_bow.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_v, y_train, epochs=50,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model = keras.models.load_model('keras_model_bow.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c47fd795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzh0lEQVR4nO3deZwU9Z3/8denj7kHBEaHU8UERblU8F7l8BePxGsNRo1rDBv1YWI0iT9dY7Ix/laTzWpiNoc/XdY1aqJLXJX8XGPiRgUJBhRQFBBFggcDCgzXMAxzdPf390d191T39Mw0TA8107yfD5qq+ta3qr/f7up+V1X3VJtzDhEREQlOKOgGiIiIHOgUxiIiIgFTGIuIiARMYSwiIhIwhbGIiEjAFMYiIiIB6zaMzewhM9tsZis7mW9m9nMzW2tmb5nZ8YVvpoiISPHK58j4YeCcLuafC4xJ3q4F7u95s0RERA4c3Yaxc24BsK2LKhcCjzrPYuAgMxtWqAaKiIgUu0J8ZjwCWO+brkuWiYiISB4iBViH5SjLeY1NM7sW71Q25eXlk0eNGlWAu/ckEglCoeL4Ppr60jcVS1+KpR+gvvRVxdKX3ujHmjVr6p1zB2eXFyKM6wB/qo4ENuaq6JybDcwGmDJlilu6dGkB7t4zf/58pk2bVrD1BUl96ZuKpS/F0g9QX/qqYulLb/TDzD7MVV6IyH8G+FLyW9UnAzudcx8XYL0iIiIHhG6PjM3sP4FpQI2Z1QHfB6IAzrkHgOeAzwJrgSZgVm81VkREpBh1G8bOucu7me+A6wvWIhERkQNM//+EXUREpJ9TGIuIiARMYSwiIhIwhbGIiEjACvF3xiIHFOccOAeJhDd0zrvKTXaZV5hR7n3fkY5l/rr+Osnx9LraJzrUbV93uqU564Y3bqTlvffaO2TW/bhXkHs03+U7rK+b+8g5O7MgtG0bbRs3Zj1OKTker+xK6eWyH7vsCfP+hUIQCoGZ12+z9DRm3vxUGV3XN/+yoRAkErjWVlwiAfE4LpHAxWJeeTzubS/J8oxh8tZhfjwBCW/o4rH2bdNCWDjVrhAWMgiFIZRsfyicLAt1rBv29ScUgnC4vR++5UINDcS2bGl/rWTdnANcIqssV73ka8MlOpalnhbf49/hccW8Nvkf++wyC3W6Htu9G+dc5jbeSxTGeXKxGLFt23DNzbhYPL2RE4+1D7NfPKkXSTyrbiKOi8VxiRx1EnEq3nuP+tWrfXWSL6Z4wpvOKI/7XoC51u9bNqNuHOcSyQ3dFyL4XwC+wKCbsmR5dtnBba2siZb08pOTGWLOX1bA+bXOsbp3e7Jf1ADrgm5EgRyM9zeVxaAWeCfoRhTIwcB73dbq+w4BEmecQbi6utfv64AOYxePE9+2jdjWrcS21BPbWk+8vp5Y/VavrH4L8fqtxOrrie/Ykb3b3WuqgS2pCTOIRNr3QsPhjHFS01l12svDWCgM4RAWiWClJRCOeHvB5tsrt9QRjn8v0vIrS5Unjx6SaQw4Pvn4EwYPG+p77HxHgfiGnc13XdVrH0/vuaaHZExb5n9dzPeVW/L+kuObNm1i6NDazOUyloH0Q+AvD2Ud75n/fl3WTNe+joz1ON+Ey+qGy+x71tVoLT3fG3yyaRNDhw71jghSzyOpo4IQEMKZrzx9OcBQ5jLp8fajkIz56Z2c5E4kifQ0zttpbD/iSU6T3NkjkdzpS7TXSZX7yrZs2cLBB9dkPliW4wFMPimWenL887PL0o+jf5sycN7QGelx79G15NMTat8sLVUfnDPfU+Krnxymdv42fvwJI0aOgFAofYRpYUseveIrTx39mXekasl+hX3lodSRenI9BoTMe3xd8rFNJJI75glvxzx73Ln0Ebdz3jyc85bxL++cdxCQ2iFPeM/LIbW1ySPm5HZjIe89x3ckni6zULLMf7TuK7dQsh++9SUft/T2lfBtJ4lkmW/bae9XAhLJfqS3RV/fXPtjtGnTJu852A+KLoxdPE58xw5i9fVeiHYSrrGtW4lv25YzYK2sjEhNDZEhQ4gedijlxx/vTdcMwcrLsXDEexGkhqEwFgn7hsngC/nqpEIyV930vBCWaGPRKy9x6skngothLg7xFoi3QrwtOfSP5yrLczwRh0TMd8ueTnQzPzntfOVZho3cH896L/Ll2yGHAIkgG1MYA0ro+nfY+pGDBgKtQbeiMA4ejHfZpCIw+CCgJehW9NzgQUAovl/uqyjCuOmNNxh81w9Y84/f8wI20fEd00pLiQwZQvjgGqIjR1J+7LFEaoYQHjKESM3BRGqGEKmpITykhlBlxb59RhBvg+YGaNkJzclbS0NyvME33QDNOzLnpcYTMU4DWNLTRwXAIFIK4RIIRzOHoWjyCDnrFinLKgt3MR3uuo6FeW/d+4z59BjfkRRZR1VZ47mOtNJHu/7yHMul1p0xjq8OvnnkWa99fW+8+SbHHXtcx3ak2+lbR4c+ddVPcqwvR586tLO7+eSc/+eFCzn9tFOTRwypo4d45nh6XjzrKDbefnSRMS/uO8JIrit9VJM64gn7prPGM4Y56nZSPn/BAqZNnZa1U53js+GelLlE5i3nY9ZZedbj1aG8/TFcuXIl4ydMyNo28tm2unldWPZ4PttmJ6+5XPeXYx2vLPwzp516So7HppNbxnzX/rh0Nd/fvtTRNb7x9M32ud6ixa9ySknvn6KGIgnjcFUViUGDqB57VI5w9Yahqqp9/xB+zw7YsMy7NWzsPGTb8titLamGsgFQOgDKBkJVLdQcmZz2ytZ8uJEjx47PCtE8xiOlmeWh8L71t4A2tM5nzMnTgm5GQez8MA6HnxZ0M3osHqmE8kFBN6MwUuFcBOo3VcHYaUE3oyDaSgZC1SFBN6PHWsrW+T6m6V1FEcalY8aw4/qvcey0aT1fWSIOW96B9a9B3VKoWwL17yZnGlQenBmmA4a3j6duvmBtn5dcJo83jo2x+Rw5uQB9ERGRfqEowrhHdtd7gZu6bXgdWhu9eRVDYOQJMPESGHkiDD/OC1UREZECOrDCON4Gn6xoP+KtWwLb3/fmhSJQOx4mXe4F8KgTYNDozM/eREREekFxh3HDRt9R71LY+AbEmr15VUO9wJ0yyzvqHTYJSiqCba+IiByQiiaMQ/FW+OjVzPBtqPNmhktg2LEw5SswcgqMOhEGjNBRr4iI9AnFEcZrX+RvFl4Of07+netBh8KhJ8HIr3tHvUPHe980FhER6YOKI4wPOYa6kRdw6Kmf9z7vra4NukUiIiJ5K44wHjCMdZ+6ikOPnhZ0S0RERPaafkJRREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCVheYWxm55jZu2a21sy+nWP+QDP7bzN708xWmdmswjdVRESkOHUbxmYWBu4DzgWOAS43s2Oyql0PvO2cmwRMA35iZiUFbquIiEhRyufI+ERgrXNunXOuFZgDXJhVxwHVZmZAFbANiBW0pSIiIkXKnHNdVzCbCZzjnLs6OX0lcJJz7uu+OtXAM8BYoBq41Dn3+xzruha4FqC2tnbynDlzCtUPGhsbqaqqKtj6gqS+9E3F0pdi6QeoL31VsfSlN/oxffr0Zc65KdnlkTyWtRxl2Ql+NrAcmAF8CviTmf3ZOdeQsZBzs4HZAFOmTHHTpk3L4+7zM3/+fAq5viCpL31TsfSlWPoB6ktfVSx92Z/9yOc0dR0wyjc9EtiYVWcW8LTzrAXexztKFhERkW7kE8ZLgDFmNjr5pazL8E5J+30EnAlgZrXAUcC6QjZURESkWHV7mto5FzOzrwPPA2HgIefcKjO7Ljn/AeBO4GEzW4F3WvtW51x9L7ZbRESkaOTzmTHOueeA57LKHvCNbwTOKmzTREREDgy6ApeIiEjAFMYiIiIBUxiLiIgETGEsIiISMIWxiIhIwBTGIiIiAVMYi4iIBExhLCIiEjCFsYiISMAUxiIiIgFTGIuIiARMYSwiIhIwhbGIiEjAFMYiIiIBUxiLiIgETGEsIiISMIWxiIhIwBTGIiIiAVMYi4iIBExhLCIiEjCFsYiISMAUxiIiIgFTGIuIiARMYSwiIhIwhbGIiEjAFMYiIiIBUxiLiIgETGEsIiISMIWxiIhIwBTGIiIiAVMYi4iIBExhLCIiEjCFsYiISMAUxiIiIgFTGIuIiARMYSwiIhIwhbGIiEjAFMYiIiIBUxiLiIgETGEsIiISMIWxiIhIwBTGIiIiAVMYi4iIBCwSdANERKTn2traqKuro7m5OeimMHDgQFavXh10M3qsJ/0oKytj5MiRRKPRvOorjEVEikBdXR3V1dUcfvjhmFmgbdm1axfV1dWBtqEQ9rUfzjm2bt1KXV0do0ePzmuZvE5Tm9k5Zvauma01s293UmeamS03s1Vm9vJetFtERHqoubmZIUOGBB7EAmbGkCFD9uosRbdHxmYWBu4DPgPUAUvM7Bnn3Nu+OgcB/xc4xzn3kZkdsreNFxGRnlEQ9x17+1zkc2R8IrDWObfOOdcKzAEuzKrzReBp59xHAM65zXvVChERkQNYPmE8Aljvm65LlvkdCQwys/lmtszMvlSoBoqISP9QVVUVdBP6rXy+wJXrWNvlWM9k4EygHFhkZoudc2syVmR2LXAtQG1tLfPnz9/rBnemsbGxoOsLkvrSNxVLX4qlH6C++A0cOJBdu3YVrkH7aNeuXcTj8T7Rlp7qaT+am5vzf06dc13egFOA533TtwG3ZdX5NnCHb/o/gEu6Wu/kyZNdIc2bN6+g6wuS+tI3FUtfiqUfzqkvfm+//XZhGtIDlZWVzjnndu7c6W6++WY3btw4N378eDdnzhznnHMbN250p59+ups0aZIbN26cW7BggYvFYu6qq65K17333nuD7EKGhoaGHi2f6zkBlrocmZjPkfESYIyZjQY2AJfhfUbs9/+AX5pZBCgBTgJ+mt/ugIiIFNL/+e9VvL2xoaDrPGb4AL5//ri86j7zzDMsX76cN998k/r6ek444QTOOOMMHn/8cc4++2y++93vEo/HaWpqYvny5WzYsIGVK1cCsGPHjoK2u7/oNoydczEz+zrwPBAGHnLOrTKz65LzH3DOrTazPwJvAQngQefcyt5suIiI9E2LFi3i8ssvJxwOU1tby9SpU1myZAknnHACf//3f09bWxsXXXQRxx57LEcccQTr1q3jhhtu4HOf+xxnnXVW0M0PRF4X/XDOPQc8l1X2QNb0PcA9hWuaiIjsi3yPYHuLdza2ozPOOIMFCxbw+9//niuvvJJbbrmFL33pS7z55ps8//zz3HfffTzxxBM89NBD+7nFwdO1qUVEpKBOO+00fvvb3xKPx9myZQsLFizgxBNP5MMPP+SQQw7hmmuu4Stf+Qqvv/469fX1JBIJPv/5z3PnnXfy+uuvB938QOhymCIiUlDnn38+y5cvZ9KkSZgZd999N0OHDuWRRx7hnnvuIRqNUlVVxaOPPsqGDRuYNWsWiUQCgH/+538OuPXBUBiLiEhBNDY2At7Vp+655x7uuSfzk8urrrqKq666qsNyB+rRsJ9OU4uIiARMYSwiIhIwhbGIiEjAFMYiIiIBUxiLiIgETGEsIiISMIWxiIhIwBTGIiLSb8RisaCb0CsUxiIiUhAXXXQRkydP5sQTT2T27NkA/PGPf+T4449n0qRJnHnmmYB3cZBZs2YxYcIEJk6cyFNPPQVAVVVVel1PPvkkX/7ylwH48pe/zE033cT06dO59dZbee211zj11FM57rjjOPXUU3n33XcB7/eHb7755vR6f/GLX/Diiy/yt3/7t+n1/ulPf+Liiy/eHw/HXtEVuEREis0fvg2frCjsOodOgHN/1GWVhx56iMGDB7N582ZmzJjBhRdeyDXXXMOCBQsYPXo027ZtA+DOO+9k4MCBrFjhtXH79u3d3v2aNWt44YUXCIfDNDQ0sGDBAiKRCC+88ALf+c53eOqpp5g9ezbvv/8+b7zxBpFIhG3btjFo0CCuv/56tmzZwsEHH8yvfvUrZs2a1fPHo8AUxiIiUhA///nPmTt3LolEgvXr1zN79mzOOOMMRo8eDcDgwYMBeOGFF5gzZ056uUGDBnW77ksuuYRwOAzAzp07ueqqq3jvvfcwM9ra2tLrve6664hEIhn3d+WVV/Kb3/yGWbNmsWjRIh599NHCdbpAFMYiIsWmmyPY3jB//nxeeOEFFi1aRDwe5/zzz2fSpEnpU8h+zjnMrEO5v6y5uTljXmVlZXr8e9/7HtOnT2fu3Ll88MEHTJs2rcv1zpo1i/PPP5+ysjIuueSSdFj3JfrMWEREemznzp0MGjSIiooK1qxZw+LFi2lpaeHll1/m/fffB0ifpj7rrLP45S9/mV42dZq6traW1atXk0gkmDt3bpf3NWLECAAefvjhdPlZZ53FAw88kP6SV+r+hg8fzvDhw7nrrrvSn0P3NQpjERHpsXPOOYdYLMbEiRO56667OPnkkzn44IOZPXs2F198MZMmTeLSSy8F4B//8R/Zvn0748ePZ9KkScybNw+AH/3oR5x33nnMmDGDYcOGdXpf//AP/8Btt93GaaedRjweT5dfffXVHHrooUycOJFJkybx+OOPp+ddccUVjBo1imOOOaaXHoGe6XvH6iIi0u+Ulpbyhz/8AYBdu3ZRXV2dnnfuuedm1K2qquKRRx7psI6ZM2cyc+bMDuX+o1+AU045hTVr1qSn77zzTgAikQj33nsv9957b4d1LFy4kGuuuSb/Du1nCmMRESlqkydPprKykp/85CdBN6VTCmMRESlqy5YtC7oJ3dJnxiIiIgFTGIuIiARMYSwiIhIwhbGIiEjAFMYiIiIBUxiLiMh+5/+FpmwffPAB48eP34+tCZ7CWEREJGD6O2MRkSLzL6/9C+9se6eg6xw7eCy3nnhrp/NvvfVWDjvsML72ta8BcMcdd2BmLFiwgO3bt9PW1sZdd93FhRdeuFf329zczFe/+lWWLl2avsLW9OnTWbVqFbNmzaK1tZVEIsFTTz3F8OHD+cIXvkBdXR3xeJzvfe976Utw9nUKYxER6bHLLruMb37zm+kwfuKJJ/jjH//It771LQYMGEB9fT0nn3wyF1xwQc5fVurMfffdB8CKFSt45513OOuss1izZg0PPPAA3/jGN7jiiitobW0lHo/z3HPPMXz4cH7/+98D3g9K9BcKYxGRItPVEWxvOe6449i8eTMbN27kgw8+YNCgQQwbNoxvfetbLFiwgFAoxIYNG9i0aRNDhw7Ne70LFy7khhtuAGDs2LEcdthhrFmzhlNOOYUf/OAH1NXVcfHFFzNmzBgmTJjAzTffzK233sp5553H6aef3lvdLTh9ZiwiIgUxc+ZMnnzySZ5++mkuu+wyHnvsMbZs2cKyZctYvnw5tbW1HX6nuDvOuZzlX/ziF3nmmWcoLy/n7LPP5qWXXuLII49k2bJlTJgwgdtuu41/+qd/KkS39gsdGYuISEFcdtllXHPNNWzevJk///nPPPHEExxyyCFEo1HmzZvHhx9+uNfrPOOMM3jssceYMWMGa9as4aOPPuKoo45i3bp1HHHEEdx4442sW7eOt956i7FjxzJ48GD+7u/+jqqqqg6/9tSXKYxFRKQgxo0bx65duxg+fDjDhg3jiiuu4Pzzz2fKlCkce+yxjB07dq/X+bWvfY3rrruOCRMmEIlEePjhhyktLeW3v/0tv/nNb4hGowwdOpTbb7+dJUuWcMsttxAKhYhGo9x///290MveoTAWEZGCWbFiBbt27QKgpqaGRYsW5azX2NjY6ToOP/xwVq5cCUBZWVnOI9zbbruN2267LaPs7LPP5uyzz97HlgdLnxmLiIgETEfGIiISiBUrVnDllVdmlJWWlvLqq68G1KLgKIxFRCQQEyZMYPny5UE3o0/QaWoREZGAKYxFREQCpjAWEREJmMJYREQkYApjERHZ77r6PeMDkcJYREQOWLFYLOgmAPrTJhGRovPJD39Iy+rC/p5x6dFjGfqd73Q6v5C/Z9zY2MiFF16Yc7lHH32UH//4x5gZEydO5Ne//jWbNm3iuuuuY926dQDcf//9DB8+nPPOOy99Ja8f//jHNDY2cscddzBt2jROPfVUXnnlFS644AKOPPJI7rrrLlpbWxkyZAiPPfYYtbW1NDY2cuONN7J06VLMjO9///vs2LGDlStX8tOf/hSAf//3f2f16tXce++9PXp8FcYiItJjhfw947KyMubOndthubfffpsf/OAHvPLKK9TU1LBt2zYAbrzxRqZOncrcuXOJx+M0Njayffv2Lu9jx44dvPzyywBs376dxYsXY2Y8+OCD3H333fzkJz/h7rvvZuDAgaxYsSJdr6SkhIkTJ3L33XcTjUb51a9+xb/927/19OHLL4zN7BzgZ0AYeNA596NO6p0ALAYudc492ePWiYjIXuvqCLa3FPL3jJ1zfOc73+mw3EsvvcTMmTOpqakBYPDgwQC89NJLPProowCEw2EGDhzYbRhfeuml6fG6ujouvfRSPv74Y1pbWxk9ejQA8+fP54knnkjXGzRoEAAzZszg2Wef5eijj6atrY0JEybs5aPVUbdhbGZh4D7gM0AdsMTMnnHOvZ2j3r8Az/e4VSIi0u+kfs/4o48+6vB7xtFolMMPPzyv3zPubDnnXLdH1SmRSIREIpGezr7fysrK9PgNN9zATTfdxAUXXMD8+fO54447ADq9v6uvvpof/vCHjB07llmzZuXVnu7k8wWuE4G1zrl1zrlWYA6Q66T/DcBTwOaCtExERPqVyy67jDlz5vC73/2OmTNnsnPnzn36PePOljvzzDN54okn2Lp1K0D6NPWZZ56Z/rnEeDxOQ0MDtbW1bN68ma1bt9LS0sKzzz7b5f2NGDECgEceeSRdPmPGDH75y1+mp1NH2yeddBLr16/n8ccf5/LLL8/34elSPmE8Aljvm65LlqWZ2Qjgb4EHCtIqERHpd3L9nvHSpUuZMmUKjz32WN6/Z9zZcuPGjeO73/0uU6dOZdKkSdx0000A/OxnP2PevHlMmDCByZMns2rVKqLRKLfffjsnnXQS5513Xpf3fccdd3DJJZdw+umnp0+BA9xyyy1s376d8ePHM2nSJObNm5ee94UvfIHTTjstfeq6p8w513UFs0uAs51zVyenrwROdM7d4KvzX8BPnHOLzexh4Nlcnxmb2bXAtQC1tbWT58yZU5BOgPftu2L5uzX1pW8qlr4USz9AffEbOHAgn/70pwvYon0Xj8cJh8NBN6PHuurHJZdcwvXXX8+0adM6XX7t2rXs3Lkzo2z69OnLnHNTOlR2znV5A04BnvdN3wbcllXnfeCD5K0R71T1RV2td/Lkya6Q5s2bV9D1BUl96ZuKpS/F0g/n1Be/t99+uzANKYCGhoagm1AQufqxfft2N2bMGDdz5sxul8/1nABLXY5MzOfb1EuAMWY2GtgAXAZ8MSvQR6fGfUfGv8tj3SIicoDqj79nfNBBB7FmzZqCr7fbMHbOxczs63jfkg4DDznnVpnZdcn5+pxYRKQPcHvxbeO+oJh/z9h18xFwtrz+ztg59xzwXFZZzhB2zn15r1ogIiI9VlZWxtatWxkyZEi/CuRi5Jxj69atlJWV5b2MrsAlIlIERo4cSV1dHVu2bAm6KTQ3N+9VEPVVPelHWVkZI0eOzLu+wlhEpAhEo9H0laOCNn/+fI477rigm9Fj+7Mf+tUmERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAlYUYeycY3NTIuhmiIiI7JOiCONFf93KrQv2cP3jr/POJw1BN0dERGSvFEUYjx02gM8dEWX+O5s551//zHW/XsbKDTuDbpaIiEheiiKMB1eWMPPIEl759gxuPHMMr/y1nvN+sZCrH1nCm+t3BN08ERGRLhVFGKccVFHCTZ85koW3zuB/f+ZIlnywnQvve4WrHnqNZR9uC7p5IiIiORVVGKcMLI9yw5ljeOXbM7j1nLGs2LCTz9+/iCseXMzidVuDbp6IiEiGvMLYzM4xs3fNbK2ZfTvH/CvM7K3k7S9mNqnwTd17VaURvjrtUyy8dTrf/ezRvPtJI5fNXswX/m0Rr6ytxzkXdBNFRES6D2MzCwP3AecCxwCXm9kxWdXeB6Y65yYCdwKzC93QnqgoiXDNGUew8NbpfP/8Y/hw626uePBVPn//X5j/7maFsoiIBCqfI+MTgbXOuXXOuVZgDnChv4Jz7i/Oue3JycXAyMI2szDKomFmnTaal2+Zzp0XjeeTnc18+VdLuOi+V3jh7U0KZRERCYR1F0BmNhM4xzl3dXL6SuAk59zXO6l/MzA2VT9r3rXAtQC1tbWT58yZ08Pmt2tsbKSqqmqvloklHK9siPHsuja27HEcNiDE+UdEOb42TMisYG3bW/vSl75Kfel7iqUfoL70VcXSl97ox/Tp05c556Z0mOGc6/IGXAI86Ju+EvhFJ3WnA6uBId2td/Lkya6Q5s2bt8/Ltsbi7r+WrndT737JHXbrs+6se192zyzf4GLxROEauBd60pe+Rn3pe4qlH86pL31VsfSlN/oBLHU5MjGf09R1wCjf9EhgY3YlM5sIPAhc6JzrV19ZjoZDzJw8khdumsq/XnossUSCG/7zDc766cv87o0NxOK61KaIiPSefMJ4CTDGzEabWQlwGfCMv4KZHQo8DVzpnFtT+GbuH5FwiIuOG8H/fGsqv/zicURCIb752+X8r3tf5r+WrqdNoSwiIr0g0l0F51zMzL4OPA+EgYecc6vM7Lrk/AeA24EhwP8177PWmMt1TryfCIeM8yYO57Pjh/E/b2/i5y++xy1PvsXPXnyPc8YN5dAhFYwaVMGoweWMHFRBWTQcdJNFRKQf6zaMAZxzzwHPZZU94Bu/Gujwha3+LhQyzhk/lLPH1fLSO5u5f/5f+fXiD2mJZR4hH1JdyqjBFYwaVM6hgysYObg9rIcNLCccCu7LYCIi0vflFcYHOjPjzKNrOfPoWhIJR31jC+u3N7F+2x4+2tbE+m1NrN/exJIPtvPMmxtJ+L6gHg0bww8qT4azF9Cp8UMHVzCoIooF+M1tEREJnsJ4L4VCxiEDyjhkQBmTD+s4vzWW4OOde9qDensyrLc18fyqT9i2uzWjfmVJmFGDKxg5yAvnUYPL2bYpRsnaeqrKIlSVRqgui1JdFqE0ElJwi4gUIYVxgZVEQhw2pJLDhlTmnN/YEqNuexMfbW1i/fY9rN/W5E1v280ra+vZ0xYH4BdvvNph2UjIqC6LJEPaC+jqUm+62l+WDHF/kHvjXt3SiD7jFhHpSxTG+1lVaYSxQwcwduiADvOcc2zd3crvX1rIUeOPpbE5xq6WtuQw5g2bYzS2eMNdzW180tBM45b2ea15fOO7JByiqixCeTRMaTREWSRMWTREeUk4OZ4sj4bT88qiyTrRMKXRcHJeqtxXJ1U/uS6nq5qJiHSrKML4k92fMHf7XJo/aGb8kPGMqBrRL0/nmhk1VaUcNiDMyUcM2ad1tMTiOUO7scU/HaOxpY09rQmaY3Fa2uI0tyVobouzc08be1q96ZZYe3kssW+hGjaofPl5KksjVJSEqSyNUFkSobI0TIV/WBKmojSSnJ81rzScXMZbR09O1zvniCUc8YSjLZ5IDjOnYwlHLJEgFk/V9cbf3Ran+sNthMwIhyw9zBg3IxQiPZ6e75tuX55+uZ2KSOEVRRiv27mOBQ0LeOnllwAYVDqI8TXjM26DywYH3Mr9ozQSprQqzJCq0oKuNxZP0BxLJIM6nhHUqeGeNm9ecyyRDPg476x9n5qhI9jdEqOpNc7u1hhNLXE27mimqTXG7tY4TS3eMF/hkHnBXhKhotQ7Ao8nHG2JZJjGM8M0Fk8kA9YL3R55bVHPls8SMnKEuRfUoezxZNCnx33l5ltPyEgu1x74Yd+y27c386t1r2EGhjc/PUyXgZGcTo7ToX7Xy4dDRiQUIhwyomEjHAoRCRmRsCWH3nQ4azxVN5qe560nu24kbHzUEGdF3U5iifYdqfahtw3kLE9Nxzsp9203iYTz9a398c2YTs1PPea+et7z3Fm91LqMdz9s468L30/fbzxr201v1/7ydPt923vGuFc3tbPpnPfc+Lcb/3Zl5t+uyJr21bcc9UOp+samTc3814bXcTgSCbyhA+eSV30EEs7hnDcka9ql6qaXSw5JXTWy/bXTvj0kh1nbTCS1PYVT21dym0wuk7HNpeokt9v3NsY4NZagJNL7vzZcFGF86vBTuefQexg2cRir6lexon4FK+tXsnDDQrynHUZUjWB8zXgm1ExgfM14jh58NBXRioBb3n9EwiGqwiGqSvduk5kf2sC0aeO6rZdIOJpjcXa3xGlq9Y7im1rj7SHuC/PdLbF0vd2tcVraEt4bddiSb+AdX3jpAAhlvXB9wdAhNHwv6nDIePPNNxk3YSKJ5Btl3Hlv1HHnTSecI56gQ1ksnprnWyZBxvL+dabekLzlU29E3jLp8fSblvdmF0+NO9L3619P+gyAczS1OcJNrck3Nu8NL/Xml3pjzCin/Q2UrOns5f3lqccg44xDPJHx1wYF8ZeFBV1dyEg/55HkWQ3wh4T3OKenyZzukdVvdyjKCIiskOlspyYSMsqioay6Iczan79Ewmuzfxvz9yPhK4snEul5GfUTuevv2ZNga7whI8C9x7Z9OrVTYsnH3L8jY0ZyByGUc3nD60dqB6U1lqCpNZ51Rivz7FcskSAeb99pb4vn92Rdf3FcYbw3IhZh3JBxjBsyji8c9QUAdrft5u2tb7OyfiUr61eyYssKnv/gecB7kj910KeYUDOBcUPGMaFmAp8e9GmioWiQ3ThghUJGRUmEipIIUNij+kJpqwsz9ciDg25Gj82fP59p0/4msPtPZByp5n7zbD8CzDzK9R/txRKOFStWcuzECcmQsoyj8faASo37Qstf17ds6sxETziXeTTnP/LLCHFfvbhzLF60iKmn/43vDEGo336U4W1j04JuRpdSj396m/OfeUhuY39Z9CqVJfsnJosmjHOpjFZywtATOGHoCemy+j31rKpfxcqtK1lRv4IXP3qRp997GoDScCljB49NHz2PrxnPodWH9ssXg0hfFQoZJenA69k3+0u3vMO0Y2p73qgC8j4WAO/4LX8DSoyB5ToY2F9Sz1M41Pk2+H5VqMc7Z/kq6jDOpaa8hqmjpjJ11FTA2zuqa6zzjpzrV7CqfhVPrnmS36z+DQADSgYwvmZ8+uh5RPUIBpQMYEDJAMoj5QpqERHpsQMujLOZGaOqRzGqehTnjj4XgFgixl93/DUd0CvrV/LQyoeIu8wvGUUswoDSAVSXVKcDOjVeXVLNgNLMsvStdABV0aou98hEROTAccCHcS6RUISjBh/FUYOP4vNHfh6APbE9vLvtXTY3baahtYGG1gZ2te6ioSU5TE5vaNzgzW9pIOZiXd5PVbSqPahLB1Ad9QJ8+7btvPraq4QtTCgUIkSIkHm3sIXT46lp73RL+zDXPP8yqZslT6N5X5joOJ7SYX7q9Jt1Pd/MWNe8jmHbhlEWKaM8Up4e6rN56UucczTFmjJey6nb7rbdRENRSiOllIXLKA2XUhYp88azykrDpZSGS/vdGbOES9Aca6Y53kxzrJk9sT00x5ppijWly1Nle2J7Msab483sadvDnrhX1hJvYXfDbua+NJeKaAUVkYqMYXmkvGN5Vp2ScEmv99n7M8cYrYlW2uJt3jDRRmvcG7bF21jXvI7TE6fvlwMnhXGeyiPlHHvIsXnXd86xJ7Yn/eJOh7cvxFPlqbL1jevZtW0XO/fsZNnaZcRdnIRLZNxS3w7vT3763z/tUBYJRSgPl1MeKac8Wk5Z2Atpf2D7b/6y7PmpN8XUG2HqFg0Fc93v1HPf2NZIY2tj5rCz8dZGdrftpjXR6u0sJXfAvG+YhpJ/KuMrI2vHyleWWiaEbzy5jnAozCdbP2Hh4oWELUw4FCZiEcKhcM7pSCjSdb3kuH/YYRnfeCgU6lAnbJnr3JfnzP9629W6i11t7a8xf7DuatvV4XW4q20Xja2NHc589URqG8wV2v7x8kh5uqxuex2vL3udRCJBgkTyC0YJ4i7ujSfLUu8LqbJU/VSZv37Ge4dztCXaMoLVH6h7KxqK5n4thkvZ4XZQ11hHU1sTTbGmdIDnKxKKdBrW5dFyKiIVhCzUHpzJ8MwO1NZ4qxe4qelk8KaWycclsUuoLqne68dnbymMe4mZeRtPtILayr37gklX30T0v9j8YZ0d3P46ndWF5J+lpAI++WcaqfvJnu+/mpa/LNd4anrZ8mWMOWZM+sXYYY86tie9V70ntofdsd1sbd7aoe6+vFEaRmm4lJJwSUZIl4RLKIuUdShPzwu3z/NPv9XwFu+99R672naxu3W3N2zbTWNrY4ey1OPblYpIBVUlVVRFq7xhSRUloZKMN9aEy3xjjiViyT+lyXpjTpb536TTyyfnp+o2tzTzzofvEEvEiLs48UScmIsRS3R9Jmd/SZ3V6Szg/fMadzdy+5zb2dW6q9szUeWRcqqj1VSXeLea8hpGDxydLkt9pJS6paYrohXEErH0UV/q6LEl3pJR1hJLDuMttMRa2BPbkzEvNb6zeSeb4pvSy6fmJxIJwqt9Z7Gydqb8ZamzXrnmZZeldtBSj9mQ6JDcO7XJHYcOZal64fYd57JIGZFQ5/GR6z0snojTHG9OB/S+DDc1baIp1sTutt0454iGo5SESoiGopSEvWE0HKU8Us7A8ECvPFRCNBz15iXnp8pSy6bmp9aRGq5euZqySFkhNutuKYz7GTMjYt7TFqXvn+ptereJaYdN69E6Unv0uQI9Nd4S997sWuOtNMebaY235pz231rjrTS0NOSs2+mRwnYoCZWkQ7QyWkl1STUjq0ZSXVJNZbSyPVyj7UGbmlcdraaypJLKSGVg3xnoamcv4RLpcI4n4sRdvENo5yrPVa+zdaV2CLOXz3Ufudaf2imJuzj1bfV8etSnOwRoh3CNVhMN9+3XS3/4c6CeCIfCVIYqqYzmvm5/n/RX9ttHagpj6fPMjJJwCSXhEgaWDtwv95naAfAH97JXl3H2tLP3y+dZQQlZiFA41C929CAZYCdPC7oZIj2mMBbJwb8DUI33edGa8JqiDmIRCU7vX+NLREREuqQwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYApjEVERAKmMBYREQmYwlhERCRgCmMREZGAKYxFREQCpjAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYDlFcZmdo6ZvWtma83s2znmm5n9PDn/LTM7vvBNFRERKU7dhrGZhYH7gHOBY4DLzeyYrGrnAmOSt2uB+wvcThERkaKVz5HxicBa59w651wrMAe4MKvOhcCjzrMYOMjMhhW4rSIiIkUpnzAeAaz3Tdcly/a2joiIiOQQyaOO5Shz+1AHM7sW7zQ2QKOZvZvH/eerBqgv4PqCpL70TcXSl2LpB6gvfVWx9KU3+nFYrsJ8wrgOGOWbHgls3Ic6OOdmA7PzuM+9ZmZLnXNTemPd+5v60jcVS1+KpR+gvvRVxdKX/dmPfE5TLwHGmNloMysBLgOeyarzDPCl5LeqTwZ2Ouc+LnBbRUREilK3R8bOuZiZfR14HggDDznnVpnZdcn5DwDPAZ8F1gJNwKzea7KIiEhxyec0Nc655/AC11/2gG/cAdcXtml7rVdOfwdEfembiqUvxdIPUF/6qmLpy37rh3k5KiIiIkHR5TBFREQCVhRh3N3lOvsLMxtlZvPMbLWZrTKzbwTdpp4ws7CZvWFmzwbdlp4ws4PM7Ekzeyf53JwSdJv2lZl9K7ltrTSz/zSzsqDblC8ze8jMNpvZSl/ZYDP7k5m9lxwOCrKN+eqkL/ckt7G3zGyumR0UYBPzkqsfvnk3m5kzs5og2ra3OuuLmd2QzJdVZnZ3b91/vw/jPC/X2V/EgP/tnDsaOBm4vh/3BeAbwOqgG1EAPwP+6JwbC0yin/bJzEYANwJTnHPj8b6QeVmwrdorDwPnZJV9G3jROTcGeDE53R88TMe+/AkY75ybCKwBbtvfjdoHD9OxH5jZKOAzwEf7u0E98DBZfTGz6XhXmJzonBsH/Li37rzfhzH5Xa6zX3DOfeycez05vgvvTb9fXsnMzEYCnwMeDLotPWFmA4AzgP8AcM61Oud2BNqonokA5WYWASrIcT2Avso5twDYllV8IfBIcvwR4KL92aZ9lasvzrn/cc7FkpOL8a7X0Kd18pwA/BT4B3Jc/Kmv6qQvXwV+5JxrSdbZ3Fv3XwxhXJSX4jSzw4HjgFcDbsq++le8F2Mi4Hb01BHAFuBXyVPuD5pZZdCN2hfOuQ14e/YfAR/jXQ/gf4JtVY/Vpq5pkBweEnB7CuXvgT8E3Yh9YWYXABucc28G3ZYCOBI43cxeNbOXzeyE3rqjYgjjvC7F2Z+YWRXwFPBN51xD0O3ZW2Z2HrDZObcs6LYUQAQ4HrjfOXccsJv+cyo0Q/Lz1AuB0cBwoNLM/i7YVkk2M/su3kdWjwXdlr1lZhXAd4Hbg25LgUSAQXgfG94CPGFmuTKnx4ohjPO6FGd/YWZRvCB+zDn3dNDt2UenAReY2Qd4HxvMMLPfBNukfVYH1DnnUmconsQL5/7ofwHvO+e2OOfagKeBUwNuU09tSv1CXHLYa6cR9wczuwo4D7jC9c+/O/0U3s7em8nX/0jgdTMbGmir9l0d8HTyFwlfwzvT1ytfSCuGMM7ncp39QnKP6z+A1c65e4Nuz75yzt3mnBvpnDsc7/l4yTnXL4/AnHOfAOvN7Khk0ZnA2wE2qSc+Ak42s4rktnYm/fTLaD7PAFclx68C/l+AbekRMzsHuBW4wDnXFHR79oVzboVz7hDn3OHJ138dcHzyddQf/Q6YAWBmRwIl9NIPYPT7ME5+4SF1uc7VwBPOuVXBtmqfnQZciXckuTx5+2zQjRJuAB4zs7eAY4EfBtucfZM8un8SeB1Ygff67zdXSjKz/wQWAUeZWZ2ZfQX4EfAZM3sP79u7PwqyjfnqpC+/BKqBPyVf+w90uZI+oJN+9Eud9OUh4IjknzvNAa7qrTMWugKXiIhIwPr9kbGIiEh/pzAWEREJmMJYREQkYApjERGRgCmMRUREAqYwFhERCZjCWEREJGAKYxERkYD9f0BJsMykwpfRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Graph training and validation loss/accuracy.\"\"\"\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e87b5027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/3d/8nmtnnsj0n53170j0cb2mtdr0000gn/T/tmpafvkqtbx/assets\n",
      "Epoch 1/7\n",
      "625/625 [==============================] - 10s 14ms/step - loss: 0.4195 - accuracy: 0.8088\n",
      "Epoch 2/7\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2778 - accuracy: 0.8897\n",
      "Epoch 3/7\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2478 - accuracy: 0.9017\n",
      "Epoch 4/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2398 - accuracy: 0.9086\n",
      "Epoch 5/7\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.2228 - accuracy: 0.9147\n",
      "Epoch 6/7\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2204 - accuracy: 0.9164\n",
      "Epoch 7/7\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2182 - accuracy: 0.9167\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/3d/8nmtnnsj0n53170j0cb2mtdr0000gn/T/tmpmum1ov7v/assets\n",
      "Epoch 1/7\n",
      "625/625 [==============================] - 10s 14ms/step - loss: 0.4224 - accuracy: 0.8087\n",
      "Epoch 2/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2797 - accuracy: 0.8911\n",
      "Epoch 3/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2521 - accuracy: 0.9015\n",
      "Epoch 4/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2347 - accuracy: 0.9098\n",
      "Epoch 5/7\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2290 - accuracy: 0.9104\n",
      "Epoch 6/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2195 - accuracy: 0.9156\n",
      "Epoch 7/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2208 - accuracy: 0.9153\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/3d/8nmtnnsj0n53170j0cb2mtdr0000gn/T/tmp_2zs3813/assets\n",
      "Epoch 1/7\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4309 - accuracy: 0.8022\n",
      "Epoch 2/7\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2875 - accuracy: 0.8862\n",
      "Epoch 3/7\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.2548 - accuracy: 0.9014\n",
      "Epoch 4/7\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2422 - accuracy: 0.9079\n",
      "Epoch 5/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2303 - accuracy: 0.9121\n",
      "Epoch 6/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2233 - accuracy: 0.9146\n",
      "Epoch 7/7\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2243 - accuracy: 0.9146\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "\"\"\"To get an idea of what kinds of things the model fails at predicting correctly,\n",
    "we'll get clean predictions on some of the training set here,\n",
    "and below, we'll evaluate and look at some misclassified instances.\"\"\"\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "#Make sure we have the sampled version of X_train_v.\n",
    "vectorize_and_select_pipeline.set_params(vectorize__ngram_range=(1,2),\n",
    "                                         vectorize__tokenizer=tokenizer_simple,\n",
    "                                         select__k=20000)\n",
    "X_train_v = vectorize_and_select_pipeline.fit_transform(X_sampled, y_sampled)\n",
    "X_train_v = X_train_v.toarray()\n",
    "\n",
    "keras_clf = KerasClassifier(model=build_mlp_model(n_hidden=3, n_neurons=30, dropout_rate=0.3, input_shape=20000,),\n",
    "                            epochs=7, #It looked like this was the epoch with peak validation above (this might overfit here, but should be close to best).\n",
    "                            verbose=1)\n",
    "\n",
    "y_train_pred = cross_val_predict(keras_clf, X_train_v, y_sampled, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f3d7194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated training evaluation: \n",
      "\n",
      " Confusion Matrix:\n",
      " [[16534  1228]\n",
      " [ 1233 11005]] \n",
      "\n",
      " Precision:  0.8996157933458677 \n",
      " Recall:     0.8992482431769897 \n",
      " F_1:        0.899431980711863\n"
     ]
    }
   ],
   "source": [
    "print('Cross-validated training evaluation:', '\\n\\n',\n",
    "      'Confusion Matrix:\\n', confusion_matrix(y_sampled, y_train_pred), '\\n\\n',\n",
    "      'Precision: ', precision_score(y_sampled, y_train_pred), '\\n',\n",
    "      'Recall:    ', recall_score(y_sampled, y_train_pred), '\\n',\n",
    "      'F_1:       ', f1_score(y_sampled, y_train_pred),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5a1a491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 4ms/step\n",
      "Non-toxic words:  ['robbery' 'robert' 'the graceful' 'the grammar' 'until have'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Look at the words most predictive of toxicity. (Toxic words commented out for decency.)\"\"\"\n",
    "\n",
    "selection_mask = vectorize_and_select_pipeline['select'].get_support()\n",
    "words = vectorize_and_select_pipeline['vectorize'].get_feature_names_out()[selection_mask]\n",
    "\n",
    "x = np.eye(X_train_v.shape[1])\n",
    "probs = model.predict(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "num = 5\n",
    "\n",
    "non_toxic_words = words[ind[:num]]\n",
    "toxic_words = words[ind[-num:]]\n",
    "\n",
    "print('Non-toxic words: ', non_toxic_words, '\\n')\n",
    "#print('Toxic words: ', toxic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c7c0c",
   "metadata": {},
   "source": [
    "Those make sense I guess, but if you run the full thing with a higher num, you get some that completely don't make sense -- in particular, some of the most highly rated non-toxic words are definitely toxic. Maybe some of these only appear in texts with many more common and strongly predictive toxic words, so their presence alone indicates nothing to the machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f8773f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[comment_text    i found this article on crime in managua its f...\n",
       " toxic                                                           0\n",
       " Name: 1967, dtype: object,\n",
       " comment_text    \"\\n\\nTo Tha Undertaker\\nIf you want to promote...\n",
       " toxic                                                           0\n",
       " Name: 2828, dtype: object]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Prints the text and toxicity tag for posts containing word. Replace word with an offensive word the machine thought\n",
    "highly predictive of non-toxicity to see that my hypothesis above might be correct.\"\"\"\n",
    "\n",
    "word = 'robbery'\n",
    "\n",
    "[full_data[['comment_text', 'toxic']].iloc[row_num]\n",
    " for row_num in full_data.index\n",
    " if (word in full_data['comment_text'][row_num])][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a3a5f629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "094b31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Look at some misclassified instances (using cross-validated clean training predictions).\"\"\"\n",
    "\n",
    "toxic_marked_clean = X_sampled[(y_sampled == 1) & (y_train_pred == 0)]\n",
    "clean_marked_toxic = X_sampled[(y_sampled == 0) & (y_train_pred == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "309f4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Entertaining... (commented out for decency)\"\"\";\n",
    "\n",
    "#pd.options.display.max_colwidth = 1000\n",
    "#toxic_marked_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8bf2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Also somewhat entertaining... (and also commented out for decency)\"\"\";\n",
    "\n",
    "#pd.options.display.max_colwidth = 1000\n",
    "#clean_marked_toxic[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d60a78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.11747551],\n",
       "       [0.9352799 ]], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Now we'll test out some made-up examples with the fully trained model.\n",
    "The outputs can be interpreted as probabilities that the text is offensive.\"\"\"\n",
    "\n",
    "model = keras.models.load_model('keras_model_bow.h5')\n",
    "\n",
    "post1 = 'This should be fine.'\n",
    "post2 = 'This should be less fine because it call you a pretentious idiot.'\n",
    "posts = [post1, post2]\n",
    "posts_v = vectorize_and_select_pipeline.transform(posts).toarray()\n",
    "\n",
    "model.predict(posts_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bac3435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998/998 [==============================] - 4s 4ms/step\n",
      "Test set evaluation \n",
      " Confusion Matrix:\n",
      " [[28521   338]\n",
      " [  950  2106]] \n",
      "\n",
      " Precision:  0.8617021276595744 \n",
      " Recall:     0.6891361256544503 \n",
      " F_1:        0.7658181818181818\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate the model on the test set.\"\"\"\n",
    "\n",
    "X_test_v = vectorize_and_select_pipeline.transform(X_test)\n",
    "X_test_v = X_test_v.toarray()\n",
    "\n",
    "test_pred_probs = model.predict(X_test_v)\n",
    "test_pred_classes = [0 if prob < 0.5 else 1 for prob in test_pred_probs]\n",
    "\n",
    "print('Test set evaluation', '\\n',\n",
    "      'Confusion Matrix:\\n', confusion_matrix(y_test, test_pred_classes), '\\n\\n',\n",
    "      'Precision: ', precision_score(y_test, test_pred_classes), '\\n',\n",
    "      'Recall:    ', recall_score(y_test, test_pred_classes), '\\n',\n",
    "      'F_1:       ', f1_score(y_test, test_pred_classes),\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
